{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "tpBZu7AsoAeL"
      },
      "source": [
        "import pandas as pd    \n",
        "import numpy as np    \n",
        "import string    \n",
        "import os    \n",
        "      \n",
        "from keras.preprocessing.sequence import pad_sequences    \n",
        "from keras.layers import Embedding, LSTM, Dense, Dropout    \n",
        "from keras.preprocessing.text import Tokenizer    \n",
        "from keras.callbacks import EarlyStopping    \n",
        "from keras.models import Sequential    \n",
        "import keras.utils as ku    \n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YtizLRY1vAYg"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBv7tS73uukQ"
      },
      "source": [
        "def preprocess():\n",
        "      f = open(\"way_station.txt\", \"r\")\n",
        "  \n",
        "      raw_lines = f.readlines()\n",
        "      proc_line = \" \".join(raw_lines).replace(\"\\n\", \"\").replace(\"\\\"\", \"\").replace(\"...\", \".\").lower()\n",
        "      no_white_line = \" \".join(proc_line.split())\n",
        "      return no_white_line.split(\".\")\n",
        "  \n",
        "      f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ydFj6Qu-vQBO"
      },
      "source": [
        "def get_sequence_of_tokens(tokenizer, corpus):\n",
        "      tokenizer.fit_on_texts(corpus)\n",
        "      total_words = len(tokenizer.word_index) + 1\n",
        "  \n",
        "      input_sequences = []\n",
        "      for line in corpus:\n",
        "          token_list = tokenizer.texts_to_sequences([line])[0]\n",
        "          for i in range(1, len(token_list)):\n",
        "              n_gram_sequence = token_list[:i+1]\n",
        "              input_sequences.append(n_gram_sequence)\n",
        "      return input_sequences, total_words\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAuh4ceYvTiA"
      },
      "source": [
        "def generate_padded_sequences(input_sequences, total_words):\n",
        "      max_sequence_len = max([len(x) for x in input_sequences])\n",
        "      input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
        "      \n",
        "      predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
        "      label = ku.to_categorical(label, num_classes=total_words)\n",
        "      return predictors, label, max_sequence_len\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vgeACZunvY4G"
      },
      "source": [
        "def create_model(max_sequence_len, total_words):\n",
        "      input_len = max_sequence_len - 1\n",
        "      model = Sequential()    \n",
        "      model.add(Embedding(total_words, 10, input_length=input_len))\n",
        "      model.add(LSTM(100))\n",
        "      model.add(Dropout(0.1))\n",
        "      model.add(Dense(total_words, activation='softmax'))\n",
        "  \n",
        "      model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "      return model\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LybX2j64vmpa"
      },
      "source": [
        "def generate_text(tokenizer, seed_text, next_words, model, max_sequence_len):\n",
        "      for _ in range(next_words):\n",
        "          token_list = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "          token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n",
        "          predicted = model.predict_classes(token_list, verbose=0)\n",
        "          \n",
        "          output_word = \"\"\n",
        "          for word,index in tokenizer.word_index.items():\n",
        "              if index == predicted:\n",
        "                  output_word = word\n",
        "                  break\n",
        "          seed_text += \" \"+output_word\n",
        "      return seed_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "ibW36cm3vrUI",
        "outputId": "7fc65147-0c10-40ba-d28e-9ed3a112982f"
      },
      "source": [
        "def main():\n",
        "      corpus = preprocess()\n",
        "      tokenizer = Tokenizer()\n",
        "      seqs, words = get_sequence_of_tokens(tokenizer, corpus)\n",
        "  \n",
        "      predictors, label, max_sequence_len = generate_padded_sequences(seqs, words)\n",
        "  \n",
        "      model = create_model(max_sequence_len, words)\n",
        "      model.summary()\n",
        "  \n",
        "      model.fit(predictors, label, epochs=50)\n",
        "\n",
        "      print(generate_text(tokenizer, \"He loved messing around with\", 20, model, max_sequence_len))\n",
        "      print(generate_text(tokenizer, \"He went down the hill and\", 20, model, max_sequence_len))\n",
        "      print(generate_text(tokenizer, \"Enoch Wallace fired and reloaded\", 20, model, max_sequence_len))\n",
        "      print(generate_text(tokenizer, \"Somewhere in the distance was the sound\", 20, model, max_sequence_len))\n",
        "      print(generate_text(tokenizer, \"The postman was not coming early today, because\", 20, model, max_sequence_len))\n",
        "      print(generate_text(tokenizer, \"The dawn was early today\", 20, model, max_sequence_len))\n",
        "  \n",
        "      print(generate_text(tokenizer, \"He loved messing around with\", 10, model, max_sequence_len))\n",
        "      print(generate_text(tokenizer, \"He went down the hill and\", 8, model, max_sequence_len))\n",
        "      print(generate_text(tokenizer, \"Enoch Wallace fired and reloaded\", 6, model, max_sequence_len))\n",
        "      print(generate_text(tokenizer, \"Somewhere in the distance was the sound\", 6, model, max_sequence_len))\n",
        "      print(generate_text(tokenizer, \"The postman was not coming early today, because\", 3, model, max_sequence_len))\n",
        "      print(generate_text(tokenizer, \"The dawn was early today\", 5, model, max_sequence_len))\n",
        "\n",
        "main()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 161, 10)           43150     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (None, 100)               44400     \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4315)              435815    \n",
            "=================================================================\n",
            "Total params: 523,365\n",
            "Trainable params: 523,365\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "1033/1033 [==============================] - 127s 123ms/step - loss: 6.4296\n",
            "Epoch 2/30\n",
            "1033/1033 [==============================] - 126s 122ms/step - loss: 6.0858\n",
            "Epoch 3/30\n",
            "1033/1033 [==============================] - 124s 120ms/step - loss: 5.9131\n",
            "Epoch 4/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 5.7369\n",
            "Epoch 5/30\n",
            "1033/1033 [==============================] - 123s 120ms/step - loss: 5.5462\n",
            "Epoch 6/30\n",
            "1033/1033 [==============================] - 124s 120ms/step - loss: 5.3537\n",
            "Epoch 7/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 5.1742\n",
            "Epoch 8/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 5.0129\n",
            "Epoch 9/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 4.8602\n",
            "Epoch 10/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 4.7243\n",
            "Epoch 11/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 4.5934\n",
            "Epoch 12/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 4.4696\n",
            "Epoch 13/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 4.3488\n",
            "Epoch 14/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 4.2331\n",
            "Epoch 15/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 4.1220\n",
            "Epoch 16/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 4.0161\n",
            "Epoch 17/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 3.9143\n",
            "Epoch 18/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 3.8155\n",
            "Epoch 19/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 3.7186\n",
            "Epoch 20/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 3.6346\n",
            "Epoch 21/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 3.5478\n",
            "Epoch 22/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 3.4666\n",
            "Epoch 23/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 3.3953\n",
            "Epoch 24/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 3.3250\n",
            "Epoch 25/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 3.2549\n",
            "Epoch 26/30\n",
            "1033/1033 [==============================] - 126s 122ms/step - loss: 3.1930\n",
            "Epoch 27/30\n",
            "1033/1033 [==============================] - 125s 121ms/step - loss: 3.1359\n",
            "Epoch 28/30\n",
            "1033/1033 [==============================] - 125s 121ms/step - loss: 3.0785\n",
            "Epoch 29/30\n",
            "1033/1033 [==============================] - 124s 120ms/step - loss: 3.0180\n",
            "Epoch 30/30\n",
            "1033/1033 [==============================] - 123s 119ms/step - loss: 2.9638\n",
            "WARNING:tensorflow:From <ipython-input-8-9d7a86b138e1>:5: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
            "Instructions for updating:\n",
            "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "He loved messing around with the sofa and it was a lot of wood and\n",
            "He went down the hill and the house and the whistling stopped open and\n",
            "Enoch Wallace fired and reloaded learn the situation said enoch is\n",
            "Somewhere in the distance was the sound of fife and drum and the\n",
            "The postman was not coming early today, because he had been\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C67JERj6wPWg",
        "outputId": "f70f8c88-ae01-446f-e91f-14f21a903ccc"
      },
      "source": [
        "%tensorflow_version 2.x\n",
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}